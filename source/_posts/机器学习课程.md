---
title: 机器学习
date: 2025-08-28 19:50:11
categories:   "课程复习"
tags: [机器学习]
toc : true
cover: "" 
---

# 0. 课程大纲

- 机器学习
  - 线性回归、逻辑回归、k-近邻、朴素贝叶斯、支持向量机、决策树、随机森林、k-Means、主成分分析…
- 深度学习
  - 卷积神经网络、循环神经网络、图神经网络、预训练模型…
- 大语言模型
- 智能软件工程

# 1. 概论

## 1.1 机器学习

### 1.1.1 机器学习定义

机器学习(Machine Learning)是一种通过模型和算法使计算机从数据中自动学习并进行预测、决策或生成内容等的技术。核心目标是让计算机在没有明确编程指令的情况下，通过对大量数据的分析，识别模式和规律，从而构建适应新数据的模型。

一个计算机程序利用经验E来学习任务T，性能是P，如果针对任务T的性能P随着经验E不断增长，则称为机器学习。

### 1.1.2 机器学习分类

按**训练过程是否使用标签**，可分为：

- 监督学习 Supervised Learning
- 无监督学习 Unsupervised Learning
- 半监督学习 Semi-Supervised Learning

> 监督学习是在已知“**正确答案(label)**”的情况下去训练模型，比如已知谁是男谁是女，让机器去学习判断。而非监督学习则是不知道具体标签或者只有相同的标签，没有预设的正确答案，机器只能**自行去划分有几类**。

按**任务类型不同**，可分为：

- 回归 Regression
- 分类 Classification
- 聚类 Clustering
- 降维 Dimensionality Reduction
- 生成 Generation
- …

按**集成策略不同**，可分为：

- 单学习器  Single-Model Learning
- 集成学习器 Ensemble Learning

> 单学习器通常只有一个模型（比如一棵决策树、一个SVM），而集成学习器是由多个模型（基学习器）组合而成

按**模型规模不同**，可分为：

- 小模型 Tiny Models
- 大模型 Large Models

按**应用场景不同**，可分为：

- 异常检测 Anomaly Detection
- 推荐系统 Recommendation Systems
- 信息检索 Information Retrieval
- …

### 1.1.3 数据集划分

- 训练集+测试集=全部数据
- 训练集+验证集+测试集=全部数据

如果数据不够用，可以将数据拆分成多份，每份轮流作为训练集/测试集，迭代n次称为**n折交叉验证**

> 这里的拆分是无序的，不是按顺序拆分

## 1.2 深度学习

深度学习(Deep Learning)是机器学习的一个子集，使用人工神经网络来处理和分析数据。神经网络由计算节点组成，这些节点分布于深度学习算法中的各层。每层都包括输入层、输出层和隐藏层。当神经网络除了输入层和输出层之外还包括多个隐藏层时，便被视为深度神经网络，而深度神经网络是深度学习的基础。 

# 2. 经典机器学习

## 2.1 回归

### 2.1.1 线性回归（建议看我的另一篇机器学习的笔记）

**最小二乘法**

最小二乘法（Least Squares Method）的核心思想是寻找一组参数，使得模型预测值与实际观测值之间的残差平方和（Residual Sum of Squares，RSS） 最小。

**解析解和近似解**

解析解（Analytical Solution）：指可以直接通过严格的数学公式表示出来的解，能够给出任意自变量对应的因变量，从而精确计算出问题的解。

近似解（Approximate Solution）又称数值解（Numerical Solution）：指利用数值方法和计算机通过有限精度的运算得到的结果，其精度在可接受的范围内，但并非精确的、完整的数学表达式。

**梯度下降法**

<img src="机器学习课程/image-20250911192415255.png" alt="image-20250911192415255" style="zoom:50%;" />

**线性**：**一个函数在数学上被称为线性的，如果它具有可加性和齐次性。**
<img src="机器学习课程/image-20250911192444609.png" alt="image-20250911192444609" style="zoom:50%;" />

- **可加性体现了“分治”思想**：将一个复杂问题分解成几个更简单的子问题，然后独立解决这些子问题，最后将它们的解相加以获得最终的解。
- **齐次性要求模型具备“扩展性”**：意味着对输入进行某种缩放（乘以一个常数因子）时，输出也会按照相同的比例缩放。该性质有助于模型的扩展性和灵活性，因为基础逻辑和结构在缩放时保持不变。

### 2.1.2 多项式回归

线性回归是对现实问题的抽象和简化，现实问题通常是非线性的，需要用更复杂的多项式来建模。

<img src="机器学习课程/image-20250911192638870.png" alt="image-20250911192638870" style="zoom:50%;" />

#### 欠拟合与过拟合

- 欠拟合： 指模型在训练集、验证集和测试集上均**表现不佳**的情况。 
- 过拟合： 指模型在训练集上表现很好，到了验证和测试阶段就很差，即模型的**泛化能力很差**。

#### 正则化

正则化（Regularization）是机器学习中对原始损失函数引入额外信息，以防止过拟合和提高模型泛化性的一类方法的统称。形式上即将原目标函数变成： 原始损失函数+额外项（正则项）。

不带正则项的损失函数:
<img src="机器学习课程/image-20250911192840803.png" alt="image-20250911192840803" style="zoom:33%;" />

#### L1 正则化（也称为“Lasso回归”）

<img src="机器学习课程/image-20250911193034575.png" alt="image-20250911193034575" style="zoom:50%;" />

- λ值为0：损失函数将与原来损失函数一样，说明对参数权重β没有任何惩罚。 

- λ为无穷大：在惩罚系数λ无穷大的情况下，为保证整个结构风险函数最小化， 只能通过最小化所有权重系数β达到目的，即通过λ的惩罚降低了参数的权 重值，而在降低参数权重值的同时我们就实现了降低模型复杂度的效果。

L1正则项通过参数的绝对值之和进行约束，倾向于产生稀疏解，即部分参数为零，常用于特征选择，因此可以减少模型的复杂度。**当我们认为特征数量很多，但只有少数特征真正重要时，L1正则化非常有用。**

#### L2 正则化（也称为“岭回归”、“Tikhonov 正则化”）

<img src="机器学习课程/image-20250911193712209.png" alt="image-20250911193712209" style="zoom:50%;" />

- **惩罚项**：所有模型参数（权重 `w`）的**平方和**。 `Ω(w) = ||w||²₂ = Σ wᵢ²`

L2正则项通过参数平方和进行约束，倾向于生成平滑权重（参数接近零但不为零），常用于防止模型过拟合。

#### 弹性网络回归

<img src="机器学习课程/image-20250911193821270.png" alt="image-20250911193821270" style="zoom:50%;" />

L1正则化和L2正则化的**线性组合**

### 2.1.3 逻辑回归

逻辑（Logistic）回归不是解决“回归”问题，而是**解决“分类”问题**。 那为什么要叫回归？ 

逻辑回归将一组自变量的线性组合“回归”到一个0到1的值，这个值是 “概率”，用来支持分类。

**Sigmoid/Logistic**

<img src="机器学习课程/image-20250911194025474.png" alt="image-20250911194025474" style="zoom:50%;" />

<img src="机器学习课程/image-20250911194043148.png" alt="image-20250911194043148" style="zoom: 67%;" />

### 2.1.4 评价指标

![image-20250911194117849](机器学习课程/image-20250911194117849.png)

**Accuracy (准确率)**：**所有预测结果中，预测正确的比例**。

**Precision (精确率/查准率)**：**在所有预测为Positive的样本中，有多少是真正的Positive**。

**Recall (召回率/查全率)**：**在所有真实的Positive样本中，模型成功预测出了多少**。

**F1-Score (F1分数)**：**Precision和Recall的调和平均数**。

平均方法：

- 微平均：将所有样本汇总，计算各指标。 
- 宏平均：先针对每一类别计算各指标，再求平均。 
- 加权平均：考虑了权重的宏平均。

### 2.1.5 k-近邻

k近邻（k-Nearest Neighbor，kNN），即给定已知类别的数据集，对于新的未知实例， 在数据集中找到与该实例最邻近的k个实例 （即k个邻居），这k个实例的多数属于哪个 类，该未知实例就属于这个类。

![image-20250911194526952](机器学习课程/image-20250911194526952.png)

**基本步骤**： 

- Step 1:  确定并获取训练集中的样本的特征值 
- Step 2:  计算训练集中的样本与未知数据之间的距离 
- Step 3:  按距离降序排序，选取与当前点距离最小的k个点 
- Step 4:  计算前k个点所在类别出现的频率 
- Step 5:  以前k个点中出现频率最高的类别作为预测类别

**k值选取**

当k的取值过小时，一旦有噪声将会对预测产生比较大影响，例如取K值为1时，一旦最近一个点是噪声，那么就会出现偏差。k值的减小就意味着整体模型变得复杂，容易发生过拟合。 

当k的值取过大时，相当于用较大邻域中的实例进行预测，学习的近似误差会增大。 这时与输入目标点较远实例也会起作用，使发生错误。k值的增大就意味着整体的模型变得简单。 

k尽量取奇数，以保证在计算结果最后会产生一个较多的类别，如果取偶数可能会产生相等的情况，不利于预测。 

k的取法：常用的方法是从k=1开始，使用检验集估计分类器的误差率。重复该过程， 每次k增加1，选取产生最小误差率的k。

**距离选取**

<img src="机器学习课程/image-20250911194702424.png" alt="image-20250911194702424" style="zoom:50%;" />

### 2.1.6 朴素贝叶斯

问题描述： 已知训练集中的样本及对应的类别，问测试集中的样本属于哪一类？ 已知样本特征，问样本属于哪一类？

举例：给定一个事物的各种特征（比如一封信的词汇），判断它最可能属于哪个类别（比如是正常邮件还是垃圾邮件）。

它的核心思路非常符合直觉：**哪个类别的概率大，就选哪个**。

**贝叶斯公式**

贝叶斯定理告诉我们如何利用证据（看到的数据）来更新我们对某个事件发生的信念（概率）。
$$
P(A|B) = \frac{P(AB)}{P(B)} = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
在这个分类的语境下，我们把它“翻译”一下：

- **A** 代表 **类别（Class）**，比如 *C*=“垃圾邮件”或 *C*=“正常邮件”。
- **B** 代表 **特征（Features）**，比如一封邮件包含了“购买”、“优惠券”等词语。我们用*X*表示所有特征的集合，$X=(x_1,x_2,...,x_n)。$

那么公式就变成了：
$$
P(C|X)=\frac{P(X|C) \cdot P(C)}{P(X)}
$$

- $P(C|X)$：**后验概率（Posterior Probability）**。这是我们最终想求的——**在已知邮件包含某些特征X的条件下，它属于类别C的概率是多少**。
- $P(C)$：**先验概率（Prior Probability）**。在根本不知道邮件内容时，我们根据历史经验认为一封邮件是垃圾邮件的**初始概率**。比如100封邮件里有20封是垃圾邮件，那么 *P*(垃圾邮件)=0.2。
- $P(X|C)$：**似然度（Likelihood）**。**如果我们已经知道这是一封垃圾邮件，那么它包含特征 X（比如“购买”这个词）的概率有多大**。
- $P(X)$：**证据（Evidence）**。就是看到特征*X*出现的概率。在实际计算中，由于我们是对比同一个*X*下不同*C*的概率，*P*(*X*)对所有类别都是相同的，可以看作一个常数，所以我们通常忽略它。

**因此，我们的目标就简化成了：找出让 $P(X|C) \cdot P(C)$最大的那个类别 C。**

现在问题来了，特征*X*是一组词，比如 `[“购买”, “优惠券”, “点击”]`。计算 *P*(*X*∣*C*)意味着要计算 **“在垃圾邮件中，同时出现‘购买’、‘优惠券’、‘点击’这三个词的概率”**。

这个计算非常复杂，因为词与词之间可能存在关联（例如，“优惠券”出现时，“购买”也很可能出现）。为了简化计算，**朴素贝叶斯做出了一个强有力的、天真的（Naive）假设**：

> 所有特征（词）在给定类别的条件下，是相互独立的。

也就是说，它认为“购买”这个词在垃圾邮件中出没，跟“优惠券”出没没关系！这个假设在现实中基本不成立，但神奇的是，基于这个假设的模型在很多场景下（尤其是文本分类）效果非常好。

基于这个“朴素”的假设，复杂的联合概率就可以拆解成一个个简单概率的**乘积**：
$$
P(X|C)=P(X_1|C) \cdot P(X_2|C) \cdot \dots
$$
所以，我们最终的**分类器**公式就是：
$$
预测类别 = \text{arg}\text{max}_C[P(C)⋅∏_{i=1}^nP(x_i∣C)]
$$
那么现在，我们可以总结朴素贝叶斯计算的流程了：

- Step1，统计出**先验概率P(C)**和**似然度P(Xi|C)**
- Step2，将先验概率和各似然度相乘
- Step3，改变C，找到使得乘积最大的C
- Step4，C就是目标分类

