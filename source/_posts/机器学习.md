---
title: 机器学习
date: 2025-06-24 19:50:01
categories:   "科研"
tags: [机器学习]
toc : true
cover: "" 
---

# 机器学习

## 监督学习（Supervised Learning）

### 回归Regression

从input到output的映射算法

通过输入input和预期的output来训练模型

​	<img src="机器学习/image-20250623175607371.png" alt="image-20250623175607371" style="zoom: 50%;" />

> 我们应用学习算法，可以在这组数据中画一条直线，或者换句话说，**拟合**一条直线，根据这条线我们可以推测出，这套房子可能卖150,000。
>
> 又或许，我们不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的曲线，我们可以从这个点推测出，这套房子能卖接近200,000。

可以看出，监督学习指的就是我们给学习算法一个**数据集**。这个数据集由“**正确答案（label）**”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价。然后运用学习算法，算出更多的正确答案。

用术语来讲，这叫做**回归（Regression）**问题。

### 分类Classification

​	<img src="机器学习/image-20250623180157790.png" alt="image-20250623180157790" style="zoom: 50%;" />

> 假设说你想通过查看病历来推测乳腺癌良性与否，如果是恶性则记为1，不是恶性，或者说良性记为0。

用术语来讲，这是一个**分类（Classification）**问题。

分类指的是，我们试着推测出离散的输出值：0或1，良性或恶性，而事实上在分类问题中，输出可能不止两个值。

比如说可能有三种乳腺癌，所以你希望预测离散输出 0、1、2、3。现在我用不同的符号来表示这些数据。良性的肿瘤改成用 O 表示，恶性的继续用 X 表示。

​	<img src="机器学习/image-20250623180909704.png" alt="image-20250623180909704" style="zoom:50%;" />

> 特征可能不止一两种，在其他机器学习问题中，我们通常有更多的特征。上图中列举了总共5种不同的特征，坐标轴上的两种和右边的3种。我们以后会讲一个算法，叫**支持向量机**，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。

## 无监督学习（Unsupervised Learning）

### 聚类算法（clustering algorithm）

对于监督学习里的每条数据， 我们已经清楚地知道，训练集对应的正确答案。

​	<img src="机器学习/image-20250623181308819.png" alt="image-20250623181308819" style="zoom:50%;" />

> 如图表所示，监督学习的数据集中每条数据都已经标明是阴性或阳性，即是良性或恶性肿瘤。

而在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即**无监督学习中没有任何的标签**或者是**有相同的标签**。

在上图中，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做**聚类算法（clustering algorithm）**。

​	<img src="机器学习/image-20250623182013649.png" alt="image-20250623182013649" style="zoom:50%;" />

> 上图是一个DNA微观数据的例子，行代表基因类型，列代表不同个体。

技术上，你要分析多少特定基因已经表达。所以这些颜色，红，绿，灰等等颜色，这些颜色展示了相应的程度，即不同的个体是否有着一个特定的基因。你能做的就是运行一个聚类算法，把个体聚类到不同的类或不同类型的组（人）。

我们没有提前告知算法一些信息，比如，这是第一类的人，那些是第二类的人，还有第三类，等等。我不知道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要**自动地聚类那些个体到各个类**。因为我们没有给算法正确答案来回应数据集中的数据，所 以这就是无监督学习。

### 无监督学习的应用

用于组织大型计算机集群：让那些机器协同工作

社交网络的分析：自动地给出朋友的分组

市场分割：自动地发现市场分类，并自动地把顾客划分到不同的细分市场中

**鸡尾酒宴问题**：多人同时在聊天，声音彼此重叠，通过算法区分出两个音频资源， 这两个可以合成或合并成之前的录音

​	<img src="机器学习/image-20250623182918268.png" alt="image-20250623182918268" style="zoom:50%;" />

# 线性回归算法

## 单变量线性回归

我们将数据以及数据的“正确答案”称为训练集，用小写的𝑚来表示训练样本的数目

以之前的房屋交易问题为例，假使我们回归问题的训练集（Training Set）如下表所示： 

​	<img src="机器学习/image-20250623184724806.png" alt="image-20250623184724806" style="zoom:50%;" />

$m$ 代表训练集中实例的数量

$x$ 代表特征/输入变量

$y$ 代表目标变量/输出变量

$(x,y)$ 代表训练集中的实例

$(x^{(i)},y^{(i)})$ 代表第𝑖个观察实例

$h$ 代表学习算法的解决方案或函数，也称为**假设（hypothesis）** 

​	<img src="机器学习/image-20250623184906289.png" alt="image-20250623184906289" style="zoom:50%;" />

这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格，我们把它喂给我们的学习算法，学习算法工作了，然后输出一个函数，通常表示为小写 ℎ  表示。ℎ 代表hypothesis(假设)，表示一个函数，输入是房屋尺寸大小。因此 ℎ 根据输入的 𝑥值来得出 𝑦 值，𝑦 值对应房子的价格。因此，**ℎ 是一个从𝑥到𝑦的函数映射。**

我们该如何表达ℎ呢？

一种可能的表达方式为：
$$
ℎ_𝜃(𝑥)=𝜃_0+𝜃_1𝑥
$$
因为**只含有一个特征/输入变量**，因此这样的问题叫作**单变量线性回归**问题。

## 代价函数（Cost Function）

在定义了上述的单变量线性回归表达式后，首要的问题是选择合适的**参数（parameters）**$\theta_0$和$\theta_1$。

我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差（modeling error）**。

​	<img src="机器学习/image-20250623185505639.png" alt="image-20250623185505639" style="zoom:50%;" />

我们的目标便是选择出可以使得**建模误差的平方和**能够最小的模型参数。 即使得**代价函数**：
$$
J(\theta_0,\theta_1)=\frac{1}{2m} \sum^{m}_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$
**最小**。

我们绘制一个等高线图，三个坐标分别为$𝜃_0$和$𝜃_1$和$𝐽(𝜃_0,𝜃_1)$：

​	<img src="机器学习/image-20250623185759684.png" alt="image-20250623185759684" style="zoom:50%;" />

可以发现三维空间中存在一个使$𝐽(𝜃_0,𝜃_1)$最小的点

代价函数也被称作**平方误差函数**，有时也被称为平方误差代价函数。

## 梯度下降（Gradient Descent）

梯度下降是一个用来**求函数最小值**的算法，我们将使用梯度下降算法来求出代价函数$𝐽(𝜃_0,𝜃_1)$的最小值。

**梯度下降背后的思想是**：开始时我们**随机选择**一个参数的组合($𝜃_0,𝜃_1,......,𝜃_𝑛$)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个**局部最小值（local minimum）**，因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是**全局最小值（global minimum）**，**选择不同的初始参数组合，可能会找到不同的局部最小值。**

​	<img src="机器学习/image-20250623190237252.png" alt="image-20250623190237252" style="zoom:67%;" />

**批量梯度下降（batch gradient descent）**算法的公式为：

​	<img src="机器学习/image-20250623203408804.png" alt="image-20250623203408804" style="zoom: 67%;" />

> 微积分的知识：这里实际上是在求$𝐽(𝜃_0,𝜃_1)$对于($\theta_0$,$\theta_1$)的**偏导数**

其中𝑎是**学习率（learning rate）**，它决定了我们沿着能让代价函数下降程度最大的方向**向下迈出的步子有多大**，在批量梯度下降中，我们每一次都**同时让所有的参数减去学习速率乘以代价函数的导数**。

梯度下降通过计算代价函数关于参数$(θ_0,θ_1)$的**梯度**（即偏导数），并沿**梯度反方向**更新参数，逐步逼近代价函数的最小值。

## 梯度下降的直观理解

​	<img src="机器学习/image-20250623193132476.png" alt="image-20250623193132476" style="zoom:50%;" />

对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的$𝜃_1$，$𝜃_1$更新后等于$𝜃_1$减去一个正数乘以𝑎。

​	如果𝑎太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动， 去努力接近最低点，这样就需要很多步才能到达最低点，所以如果𝑎太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。  

​	如果𝑎太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果𝑎太大，它会导致无法收敛，甚至发散。

> 如果我们预先把$𝜃_1$放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？
>
> 局部最优点的导数将等于零,使得$𝜃_1$不再改变

​	<img src="机器学习/image-20250623200953198.png" alt="image-20250623200953198" style="zoom:50%;" />

​	在这个例子里，我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在第一个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，$𝜃_1$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。

​	随着梯度下降，导数值也会越来越小，相应的我每次移动的步长自然而然会变小，所以不需要额外修改$\alpha$

==于是，结合单变量线性回归、代价函数、梯度下降的学习，我们得出了第一种机器学习算法，即线性回归算法。==

## 梯度下降的线性回归

我们将上述的线性回归算法、代价函数、梯度下降的公式合并，得到：

![image-20250623203337816](机器学习/image-20250623203337816.png)

**”批量梯度下降”**，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算。

因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。

# 多变量线性回归

